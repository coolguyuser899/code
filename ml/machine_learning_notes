Introduction To Tensorflow from https://www.youtube.com/watch?v=FQ660T4uu7k

real word -> raw data -> data is processed -> clean data -> exploratory data analysys
                                                        | -> exploratory data analysys
                                                        | -> machine learning algorithms statistical models
                                                            |-> communication, visulizations, report findings (make decisions)
                                                            |-> build data product
                                                                |-> back link to real world
                                                                    |-> real world

tensorflow 1.4 works with python3.5, you see runtime not compatible warning when running python 3.6

python3.6 -m pip3 install tensorflow

tensor: a n-dimensional matrix
3 (rank 0)
[1., 2., 3.] (rank 1 - shape[3])
[[1.,2.,3],[4.,5.,6]] (rank 2 - shape[2,3])
tensorboard - graph visualization
https://www.tensorflow.org/install/

import tensorflow as tf

how tensorflow works in examples
http://bcomposes.com/2015/11/26/simple-end-to-end-tensorflow-examples

https://www.tensorflow.org/tutorials/image_recognition   imagenet

http://workpiles.com/2016/06/ccb9-prototype2-recognition_system
japanese sorting cucumbers for different market using raspberry pi
・TensorFlow 0.9.0
・wxPython
・openCV
・pySerial

tensorflow for mobile
tensorflow serving   #serving backend processing, operating, optimizing
tensorflow research cloud  #cloud gpu based
distributed tensorflow  #can connect to remote machine port

####
####tf get started
$ python3
Python 3.5.1 (v3.5.1:37a07cee5969, Dec  5 2015, 21:12:44)
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> import tensorflow as tf
>>> node1 = tf.constant(3.0, tf.float32)
>>> node2 = tf.constant(4.0)
>>> print(node1, node2)
Tensor("Const:0", shape=(), dtype=float32) Tensor("Const_1:0", shape=(), dtype=float32)
>>> sess = tf.Session()
>>> print(sess.run([node1, node2]))
[3.0, 4.0]
>>> node3 = tf.add(node1, node2)
>>> print("node3", node3)
node3 Tensor("Add:0", shape=(), dtype=float32)
>>> print("sess.run(node3): ", sess.run(node3))
sess.run(node3):  7.0
>>> print("sess.run(node1 + node2):", sess.run(node1 + node2))
sess.run(node1 + node2): 7.0
>>> a = tf.placeholder(tf.float32)
>>> b = tf.placeholder(tf.float32)
>>> adder_node = a + b
>>> print(sess.run(adder_node, {a:3, b:4.5}))
7.5
>>> print(sess.run(adder_node, {a:[1,3], b:[2,4]}))
[ 3.  7.]
>>> add_and_triple = adder_node * 3
>>> print(sess.run(add_and_triple, {a:3, b:4.5})
... )
22.5

###tf board
>>> import tensorflow as tf
>>> a = tf.constant(5, name="input_a")
>>> b = tf.constant(3, name="input_b")
>>> c = tf.multiply(a, b, name="multiply_c")
>>> d = tf.add(a,b, name="add_d")
>>> e = tf.add(c,d, name="add_e")
>>> sess = tf.Session()
>>> output = sess.run(e)
>>> writer = tf.train.SummaryWriter('./my_graph', sess.graph)  ##tf 1.2 changed to tf.summary.FileWriter
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: module 'tensorflow.python.training.training' has no attribute 'SummaryWriter'
>>> writer = tf.summary.FileWriter('./my_graph', sess.graph)
>>> writer.close()
>>> sess.close()
>>>

####tf board to see graph
tensorboard --logdir=my_graph/
Starting TensorBoard b'47' at http://0.0.0.0:6006


## linear model can be used for predictive purpose, classifier
#Weight = tf.Variable([.3], tf.float32)
#bias = tf.Variable([-.3], tf.float32)
#x = tf.placeholder(tf.float32)
#linear_model = W * x + b


>>> W = tf.Variable([.3], tf.float32)
>>> b = tf.Variable([-.3], tf.float32)
>>> x = tf.placeholder(tf.float32)
>>> linear_model = W * x + b
>>> init = tf.global_variables_initializer()
>>> sess = tf.Session()
>>> sess.run(init)
>>> print(sess.run(linear_model, {x:[1,2,3,4]}))
[ 0.          0.30000001  0.60000002  0.90000004]

>>> y = tf.placeholder(tf.float32)
>>> squared_deltas = tf.square(linear_model - y)
>>> loss = tf.reduce_sum(squared_deltas)
>>> print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))   #show result not optimal
23.66

>>> fixW = tf.assign(W, [-1.])
>>> fixb = tf.assign(b, [1.])
>>> sess.run([fixW, fixb])
[array([-1.], dtype=float32), array([ 1.], dtype=float32)]
>>> print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))
0.0      #show best result

>>> optimizer = tf.train.GradientDescentOptimizer(0.01)   #use train gradient to find closest values, -0.99, 0.99
>>> train = optimizer(minimize(loss)
>>> train = optimizer.minimize(loss)
>>> sess.run(init)
>>> for i in range(1000):        #to run 1000 times
...     sess.run(train, {x:[1,2,3,4], y:[0,-1,-2,-3]})
...
>>> print(sess.run([W,b]))
[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]

##image recognition imagenet
#https://tensorflow.org/tutorials/imgage_recognition, inception v3 model
mkdir tf_imagenet
git clone https://github.com/tensorflow/models.git tensorflow-models
cd ./tutorials/image/imagenet/classify_image.py
python3.6 classify_image.py

/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)
>> Downloading inception-2015-12-05.tgz 100.0%
Successfully downloaded inception-2015-12-05.tgz 88931400 bytes.
2018-01-01 12:54:01.883305: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-01-01 12:54:03.584564: W tensorflow/core/framework/op_def_util.cc:334] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().
giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca (score = 0.89107)      ##########testing image high score
indri, indris, Indri indri, Indri brevicaudatus (score = 0.00779)
lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens (score = 0.00296)
custard apple (score = 0.00147)
earthstar (score = 0.00117)

#### images downloaded
/tmp/imagenet/
total 362176
-rw-rw-r--  1 Marshmallows  wheel     11416  4 Dec  2015 LICENSE
-rw-rw-r--  1 Marshmallows  wheel  95673916  5 Dec  2015 classify_image_graph_def.pb
-rw-rw-r--  1 Marshmallows  wheel      2683  1 Dec  2015 cropped_panda.jpg
-rw-rw-r--  1 Marshmallows  wheel     64986 18 Nov  2015 imagenet_2012_challenge_label_map_proto.pbtxt
-rw-rw-r--  1 Marshmallows  wheel    741401 18 Nov  2015 imagenet_synset_to_human_label_map.txt
-rw-r--r--  1 Marshmallows  wheel  88931400  1 Jan 12:53 inception-2015-12-05.tgz


python3.6 classify_image.py --image_file flamingo.png
flamingo (score = 0.73916)
goose (score = 0.04539)
pelican (score = 0.01839)
black swan, Cygnus atratus (score = 0.00962)
black stork, Ciconia nigra (score = 0.00559)

python3.6 classify_image.py --image_file pug.png
Brabancon griffon (score = 0.59837)
pug, pug-dog (score = 0.35555)
French bulldog (score = 0.00599)
affenpinscher, monkey pinscher, monkey dog (score = 0.00125)
Pekinese, Pekingese, Peke (score = 0.00102)

python3.6 classify_image.py --image_file terrier.png
Norwich terrier (score = 0.83114)
Norfolk terrier (score = 0.11521)
West Highland white terrier (score = 0.00758)
Scotch terrier, Scottish terrier, Scottie (score = 0.00368)
Australian terrier (score = 0.00278)


####
distributed tensorflow  #can connect to remote machine port

import tensorflow as tf
c = tf.constant("Hello, distributed tf")
sess = tf.Session("grpc://localhost:2222")
sess.run(c)
#Specifi code runs on a GPU
with tf.device('/gpu:0')
#with tf.device("job:worker/task:7")
    ...
    train_op = ...
#Now, launch the job on 'worker7' node
with tf.Session("grpc://worker7:2222") as sess:
    for _ in range(10000):
        sess.run(train_op)

####
quit()

####linear data and graph
python3.6 softmax.py --train simdata/linear_data_train.csv --test simdata/linear_data_eval.csv --num_epochs 1 --verbose True

##moon data run twice
python3.6 softmax.py --train simdata/moon_data_train.csv --test simdata/moon_data_eval.csv --num_epochs 10

####Learning materials

Michael Nielsen's book
Andrew Ng's class
How Convolutional Neural Networks work - https://www.youtube.com/watch?v=FmpDIaiMIeA


"""
each connection has a different weight value
bias is the property of the neuron
layer is a stack of neurons, input, hidden, output layers
stride is the pixel when slide image window
convolutional layer: a mathematical operation to filter patterns, most important is filter size
pooling layer: to reduce spatial size (width and height, not depth), with stride of 2, size reduced by half
fully connected layer: each neuron in a layer receives input from all the neurons in the previous layer
weights(w) and bias(b: parameters of network
learning rate: backward propagation during gradient descent, how fast do we change the training parameter
cost: indicates if training is going the right direction, when cost reduces, accuracy increases
model: after training done, the parameter and architecture is saved in binary model file
inference or prediction: load model and calculate probability of new image
"""

Activation function
sigmoid function S(x) = e pow (x) / e pow (x+1)
if x close to -1 S(x) = -1
if x close to +1 S(x) = +1
if x close to 0.0 S(x) = -1 < x < +1

RELU (rectified linear unit) transfer output to either 0 or itself
f(x) = max(0, max)
if x < =0, f(x) = 0
if x > 0, f(x) = x

VGG16 is the a convolution network which won imagenet competition in 2014


# from keras.models import Sequential
# from keras.layers import Dense, Activation
#
# model = Sequential([
#     Dense(5, input_shape=(3,), activation='relu'),
#     Dense(2, activation='softmax'),
# ])
#
# # print(model)
#
# import matplotlib.pyplot as plt
# import matplotlib.image as mpimg
# image = mpimg.imread("NN.png")
# plt.imshow(image)
# plt.show()

#activation function, defines output giving input, output = activation(weighted sum of inputs)
# from keras.models import Sequential
# from keras.layers import Dense, Activation
#
# model = Sequential ([
#     Dense(5, input_shape=(3,), activation='relu')
# ])
#
# ##same as the following code
# model = Sequential()
# model.add(Dense(5, input_shape=3,))
# model.add(Activation('relu'))

#Training a neural network
#SGD stochastic gradient descent, to minimize the loss (incremental gradient descent)

#Learing process of neural network
# epoch - one pass of data
# learning rate: between 0.1 and 0.001
# d(loss) / d(weight) x 0.001
# Adam = variation of SDG for optimization

import keras
from keras import backend as K
from keras.models import Sequential
from keras.layers import Activation
from keras.layers.core import Dense
from keras.optimizers import Adam
from keras.metrics import categorical_crossentropy
#
# #build a model, input layer 1, 2 hidden layer
# model = Sequential([
#     Dense(16, input_shape=(1,), activation='relu'),
#     Dense(32, activation='relu'),
#     Dense(2, activation='softmax')
# ])
#
# #Adam optimizer, learning rate = 0.0001, loss = type, matrics = what is to print out
# model.compile(Adam(lr=.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# for a single-input model with 2 classes (binary):

#### keras example
# model = Sequential()
# model.add(Dense(1, input_dim=784, activation='softmax'))
# model.compile(optimizer='rmsprop',
#               loss='binary_crossentropy',
#               metrics=['accuracy'])
#
# # generate dummy data
# import numpy as np
# scaled_train_samples = np.random.random((1000, 784))
# train_labels = np.random.randint(2, size=(1000, 1))
#
# #np array training data, labels, how many piece data feed at once, how many passes
# model.fit(scaled_train_samples, train_labels, batch_size=10, epochs=20, shuffle=True, verbose=2)

####loss function
# minimize lost with continuous update weights and model during training
# MSE mean squared error


##Different dataset used in train, test, validation
#train data set, used to compute again and again
#overfitting, model is good at training data predication, but not good at validation data set
#testset, different from train and validation data, has to be unlabeled, without knowing what it is

# import numpy as np
# scaled_train_samples = np.random.random_sample((10,))
# train_labels = np.random.randint(2, size=(10, 1))
# print(scaled_train_samples, train_labels)
#
# model = Sequential([
#     Dense(16, input_shape=(1,), activation='relu'),
#     Dense(32, activation='relu'),
#     Dense(2, activation='softmax')
# ])
#
# #Adam optimizer, learning rate = 0.0001, loss = type, matrics = what is to print out
# model.compile(Adam(lr=.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
# #use 20% of training set as validation set
# model.fit(scaled_train_samples, train_labels, validation_split=0.20, batch_size=10, epochs=20, shuffle=True, verbose=2)

# ##or use separate set
# valid_set = [(sample, label),(sample, label),....,(sample, label),(sample, label)]
# model.fit(scaled_train_samples, train_labels, validation_data=valid_set, batch_size=10, epochs=20, shuffle=True, verbose=2)


##overfitting, model is good at classfifying and predicting training set, but not good at data which is trained on or test set
# if validation accuracy is worse then training, overfitting indicator
# model is not generalized
# solution, 1) add more data to training set
# 2) data augmentation (rotation, scaling, flip),
# 3) reduce complexity of model
# 4) dropout of nodes at certain layer

##underfitting
# model is not good at data which is trained on
# solution: 1) add complexity to model, model is too simple, increase neurons or layers
# 2) add more features to the training set
# 3) reduce dropout, define percentage of dropout

### Regularization in neural network, reduce overfitting or reduce variant by not penalizing complexity
# L2 regularization loss + x
# from keras import regularizers
# model = Sequential([
#     Dense(16, input_shape=(1,), activation='relu'),
#     Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
#     Dense(16, input_shape=(1,), activation='sigmoid'),
# ])

#### learning rate, often between 0.01 to 0.0001
# model.compile(Adam(lr=0.0001), loss='sparse_categorical_crossentrypy', metrics=['accuracy'])
# model.optimizer.lr = 0.01
# model.optimizer.lr


#### Batch size number of sample passed to network (as a group)
# value must be tested and tuned for better result
#batch_size=10   #pass data set 10 at a time

#### Fine tuning a neural network or transfer learning
#use exisiing model by remove last layer or remove/add more hidden layer for mew model

#### Data augmentation, create new data based on existing data, horizontal flip
#rotate, shift width, shift hight, zooming, varying color, flip


####predicating - have model to predict
#pass un-labelled data to match model on test data
# predictions = model.predict(scaled_test_sample, batch_size=10, verbose=0)
# for i in predictions:
#     print(i)


####Supervised learning - training and validation data are labeled
#data (sample, label) -> output, they are used to learn data
#
# import keras
# from keras import backend as K
# from keras.models import Sequential
# from keras.layers import Activation
# from keras.layers.core import Dense
# from keras.optimizers import Adam
#
# model = Sequential([
#     Dense(16, input_shape=(2,), activation='relu'),
#     Dense(32, activation='relu'),
#     Dense(2, activation='sigmoid')
# ])
#
# model.compile(Adam(lr=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
# #weight, height
# train_samples = [[150, 67], [130, 60], [200, 65], [125, 52], [230, 72], [181, 70]]
# # 0: male
# # 1: female
# train_labels = [1, 1, 0, 1, 0, 0]
# model.fit(x=train_samples, y=train_labels, batch_size=3, epochs=10, shuffle=True, verbose=2)

####Unsupervised learning
#training data is not labeled, no way to measure the accuracy
#one popular is clusterign algorithm, create learning structure
#autoencoder - handwritten number as input and reconstruct image
#original input -> encoder -> compressed representation-> reconstructed input
#why we need unsupervised learing and reconstructing image
#denoise image, and reconstruct better quality image


####Semi-supervised learning using both labeled and unlabled data
#use pseudo-labeling
#add label in predict process


####Convolutional neural network (CNNs)
#so far the most popular network for analyzing image, but it can also used ofr data classification as well
#convolutional layer use filters to detect pattern
#MLP multi-layer percepron
#hidden layer is convolutional layers
#filter slide over each 3x3 set of pixels from the input itself until it slid over every 3x3 block
#this sliding is referred to as convolving


#Why image is always divided by 255

Typically, RGB values are encoded as 8-bit integers, which range from 0 to 255.
It's an industry standard to think of 0.0f as black and 1.0f as white (max brightness).
To convert [0, 255] to [0.0f, 1.0f] all you have to do is divide by 255.0f.

#
Most of the advanced deep learning models like VGG, ResNet etc. require square images as input,
usually with a pixel size of 224x224.
Since, the VGG model is trained on all the image resized to 224x224 pixels,
so for any new image that the model will make predictions upon has to be resized to these pixel values.


